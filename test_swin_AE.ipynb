{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "\n",
    "from torch.utils.data.dataloader import T\n",
    "import numpy as np\n",
    "# TODO fix mkl problem \n",
    "# os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import pywick \n",
    "from pywick.models.segmentation import deeplab_v3_plus\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from utils.util import json_file_to_dict_args, json_file_to_pyobj,get_tags\n",
    "from dataio.loader import get_dataset, get_dataset_path\n",
    "from dataio.transformation import get_dataset_transformation\n",
    "from torch.utils.data import DataLoader, dataset\n",
    "from pywick.modules import ModuleTrainer\n",
    "import pywick.metrics as pwm\n",
    "from utils.error_logger import ErrorLogger\n",
    "import torch\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from utils.visualiser import Visualiser\n",
    "from utils.error_logger import ErrorLogger\n",
    "import pytorch_lightning as pl\n",
    "from models import get_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "class DivideIntoPatches:\n",
    "    def __init__(self, patch_size):\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        height, width = img.shape[-2:]\n",
    "        patches = []\n",
    "        for i in range(0, height - self.patch_size + 1, self.patch_size):\n",
    "            for j in range(0, width - self.patch_size + 1, self.patch_size):\n",
    "                patches.append(img[ :, i:i + self.patch_size, j:j + self.patch_size])\n",
    "        return torch.stack(patches, dim=0)\n",
    "class ResizePatches:\n",
    "    def __init__(self,pz):\n",
    "        self.patch_size = pz\n",
    "    def __call__(self, img):\n",
    "        imgs = img.reshape(-1,img.shape[-3],img.shape[-2],img.shape[-1])\n",
    "        return torch.nn.functional.interpolate(imgs, size=(self.patch_size,self.patch_size), mode='bilinear', align_corners=False)\n",
    "def get_data(pz = 80, img_size=224, bs=None,split=\"train\",pz_=None):\n",
    "    patch_size = pz\n",
    "    img_size = img_size\n",
    "    num_patches = (img_size // patch_size) ** 2\n",
    "    bs = 90 // num_patches if bs is None else bs\n",
    "    if num_patches <5:\n",
    "        bs=bs//8\n",
    "    bs = 1 if bs == 0 else bs\n",
    "    print(\"config:\\n\", \"patch_size:\", patch_size, \"img_size:\", img_size, \"num_patches:\", num_patches, \"bs:\", bs)\n",
    "    if pz_ is None:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ConvertImageDtype(torch.float),\n",
    "            DivideIntoPatches(patch_size=patch_size), # takes an image tensor and returns a list of patches stacked as (H // patch_size **2 x H x W x C)\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        DivideIntoPatches(patch_size=patch_size), # takes an image tensor and returns a list of patches stacked as (H // patch_size **2 x H x W x C)\n",
    "        ResizePatches(pz=pz_)\n",
    "        ])\n",
    "\n",
    "    # data = wss_dataset_class(\"/home/uz1/data/wsss/train/1.training\", 'all',\n",
    "                            #  transform)\n",
    "    # data = HDF5Dataset(\"/home/uz1/DATA!/pcam/pcam/training_split.h5\",\"/home/uz1/DATA!/pcam/Labels/Labels/camelyonpatch_level_2_split_train_y.h5\",transform=transform)\n",
    "    from medmnist.dataset import PathMNIST, BreastMNIST,OCTMNIST,ChestMNIST,PneumoniaMNIST,DermaMNIST,RetinaMNIST,BloodMNIST,TissueMNIST,OrganAMNIST,OrganCMNIST,OrganSMNIST\n",
    "    # using a unified ataset of medmnist\n",
    "    data = PathMNIST(root='/home/uz1/DATA!/medmnist', split=split,transform=transform)\n",
    "    loader = DataLoader(data, batch_size=bs, drop_last=True, num_workers=14)\n",
    "    return data,loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:35: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:93: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/pl_bolts/losses/self_supervised_learning.py:234: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n",
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/pl_bolts/datamodules/experience_source.py:18: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"gym\")\n"
     ]
    }
   ],
   "source": [
    "from models.networks.swin_transformer import SwinTransformer\n",
    "from torchvision.models import swin_transformer\n",
    "from pl_bolts.models.autoencoders.components import (\n",
    "    resnet18_decoder,\n",
    "    resnet18_encoder,\n",
    "    ResNetDecoder,\n",
    "    DecoderBlock,\n",
    ")\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class Interpolate(nn.Module):\n",
    "    \"\"\"nn.Module wrapper for F.interpolate.\"\"\"\n",
    "\n",
    "    def __init__(self, size=None, scale_factor=None):\n",
    "        super().__init__()\n",
    "        self.size, self.scale_factor = size, scale_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.interpolate(x, size=self.size, scale_factor=self.scale_factor)\n",
    "def resize_conv1x1(in_planes, out_planes, scale=1):\n",
    "    \"\"\"upsample + 1x1 convolution with padding to avoid checkerboard artifact.\"\"\"\n",
    "    if scale == 1:\n",
    "        return conv1x1(in_planes, out_planes)\n",
    "    return nn.Sequential(Interpolate(scale_factor=scale), conv1x1(in_planes, out_planes))\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution.\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class Resdecoder(nn.Module):\n",
    "    def __init__(self, x,latent_dim=256, input_height=224, first_conv=False, maxpool1=False):\n",
    "        # super().__init__(   DecoderBlock, [2, 2, 2, 2],latent_dim=latent_dim, input_height=input_height, first_conv=first_conv, maxpool1=maxpool1)\n",
    "        super().__init__()\n",
    "        self.block = DecoderBlock\n",
    "        self.latent_dim = latent_dim\n",
    "        self.expansion = self.block.expansion\n",
    "        self.first_conv = first_conv\n",
    "        self.maxpool1 = maxpool1\n",
    "        self.input_height = input_height\n",
    "        self.layers = [2, 2, 2, 2]\n",
    "        self.upscale_factor = 8\n",
    "        \n",
    "\n",
    "        _, self.c, self.h, self.w = x.shape\n",
    "        block = self.block\n",
    "        layers = self.layers\n",
    "        out_put_shape = self.c # * self.h * self.w\n",
    "        self.linear = torch.nn.Linear(self.latent_dim, out_put_shape * self.h * self.w)\n",
    "        self.inplanes = out_put_shape\n",
    "\n",
    "\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], scale=2)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], scale=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], scale=2)\n",
    "\n",
    "        if self.maxpool1:\n",
    "            self.layer4 = self._make_layer(block, 64, layers[3], scale=2)\n",
    "            self.upscale_factor *= 2\n",
    "        else:\n",
    "            self.layer4 = self._make_layer(block, 64, layers[3])\n",
    "\n",
    "        if self.first_conv:\n",
    "            self.upscale = Interpolate(scale_factor=2)\n",
    "            self.upscale_factor *= 2\n",
    "        else:\n",
    "            self.upscale = Interpolate(scale_factor=1)\n",
    "\n",
    "        # interpolate after linear layer using scale factor\n",
    "        self.upscale1 = Interpolate(size=self.input_height // self.upscale_factor)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(64 * block.expansion, 3, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    def _make_layer(self, block, planes, blocks, scale=1):\n",
    "        upsample = None\n",
    "        if scale != 1 or self.inplanes != planes * block.expansion:\n",
    "            upsample = nn.Sequential(\n",
    "                resize_conv1x1(self.inplanes, planes * block.expansion, scale),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, scale, upsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "\n",
    "        x = x.view(x.size(0), self.c, self.h, self.w)\n",
    "        x = self.upscale1(x)\n",
    "        # print('in to l1',x.shape)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.upscale(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      " patch_size: 112 img_size: 224 num_patches: 4 bs: 2\n",
      "using input height  112  and latent dim  256  and enc_out_dim  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/ipykernel_launcher.py:81: UnderReviewWarning: The feature DecoderBlock is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/pl_bolts/models/autoencoders/components.py:132: UnderReviewWarning: The feature resize_conv3x3 is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.conv1 = resize_conv3x3(inplanes, inplanes)\n",
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/pl_bolts/models/autoencoders/components.py:36: UnderReviewWarning: The feature conv3x3 is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  return conv3x3(in_planes, out_planes)\n",
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/pl_bolts/models/autoencoders/components.py:37: UnderReviewWarning: The feature Interpolate is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  return nn.Sequential(Interpolate(scale_factor=scale), conv3x3(in_planes, out_planes))\n",
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/pytorch_lightning/core/module.py:378: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  \"You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(52627.7031, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, enc_out_dim=512, latent_dim=256, input_height=32, window_size=4,patch_size = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.input_height = input_height\n",
    "        print(\"using input height \",input_height,\" and latent dim \",latent_dim,\" and enc_out_dim \",enc_out_dim)\n",
    "        # self.encoder = SwinTransformerAE(img_size=input_height,patch_size = patch_size,window_size= window_size,enc_out = enc_out_dim)\n",
    "        self.encoder = swin_transformer._swin_transformer(\n",
    "                        patch_size=[patch_size, patch_size],\n",
    "                        embed_dim=96,\n",
    "                        depths=[4,6],\n",
    "                        num_heads=[12, 24],\n",
    "                        window_size=[window_size, window_size],\n",
    "                        stochastic_depth_prob=0.1,\n",
    "                        weights=None,\n",
    "                        progress=0,\n",
    "                        num_classes=enc_out_dim,\n",
    "                    )\n",
    "\n",
    "    \n",
    "\n",
    "        # distribution parameters\n",
    "        self.fc_mu = nn.Linear(enc_out_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(enc_out_dim, latent_dim)\n",
    "\n",
    "        #simulate a pass through the encoder \n",
    "        x = self.encoder.features(torch.rand(1,3,input_height, input_height )).permute(0,3,1,2)\n",
    "        self.decoder = Resdecoder(x,latent_dim=latent_dim,input_height=input_height,first_conv=False,maxpool1=False)\n",
    "\n",
    "        # for the gaussian likelihood\n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def gaussian_likelihood(self, mean, logscale, sample):\n",
    "        scale = torch.exp(logscale)\n",
    "        dist = torch.distributions.Normal(mean, scale)\n",
    "        log_pxz = dist.log_prob(sample)\n",
    "        return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "    def kl_divergence(self, z, mu, std):\n",
    "        # --------------------------\n",
    "        # Monte carlo KL divergence\n",
    "        # --------------------------\n",
    "        # 1. define the first two probabilities (in this case Normal for both)\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu),\n",
    "                                       torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        # 2. get the probabilities from the equation\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        # kl\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "        return kl\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # print(batch)\n",
    "        x, _ = batch\n",
    "        #if channels are less than 3, repeat channels\n",
    "        # print(x.shape)\n",
    "        # if x.shape[1] < 3:\n",
    "        #     x = x.repeat(1, 3, 1, 1)\n",
    "        # print('x in training step',x.shape)\n",
    "        if x.dim() >4: # b,16,3,h,w -> b*16,3,h,w\n",
    "            x = x.view(-1, x.shape[2], x.shape[3], x.shape[4])\n",
    "        # encode x to get the mu and variance parameters\n",
    "        x_encoded = self.encoder(x)\n",
    "        # print('x_encoded in training step',x_encoded.shape)\n",
    "        mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\n",
    "        # detect nan \n",
    "        if torch.isnan(mu).any() or torch.isnan(log_var).any():\n",
    "            print(\"NAN in mu or log_var\")\n",
    "            print(mu)\n",
    "            print(log_var)\n",
    "            print(x_encoded,x_encoded.shape)\n",
    "            print(x,x.shape)\n",
    "            raise ValueError(\"NAN in mu or log_var\")\n",
    "        # sample z from q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "\n",
    "        # decoded\n",
    "        # print('z in training step',z.shape)\n",
    "        x_hat = self.decoder(z)\n",
    "        # print('x_hat in training step',x_hat.shape)\n",
    "        # reconstruction loss\n",
    "        recon_loss_ = self.gaussian_likelihood(x_hat, self.log_scale, x) # old recon_loss \n",
    "        # print(recon_loss.shape)\n",
    "        recon_loss = torch.nn.MSELoss()(x_hat,x)\n",
    "        # print(recon_loss.shape)\n",
    "\n",
    "        # kl\n",
    "        kl = self.kl_divergence(z, mu, std)\n",
    "\n",
    "        # elbo\n",
    "        elbo = (kl - recon_loss_) # with old recon_loss\n",
    "        # elbo = (kl + recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "\n",
    "        self.log_dict({\n",
    "            'elbo': elbo,\n",
    "            'kl': kl.mean(),\n",
    "            'recon_loss_mse': recon_loss.mean(),\n",
    "            'recon_loss_gl': recon_loss_.mean(),\n",
    "            'reconstruction': recon_loss.mean(),\n",
    "            'kl': kl.mean(),\n",
    "        })\n",
    "\n",
    "        return elbo\n",
    "    def forward(self,batch):\n",
    "        x, _ = batch if len(batch)==2 else batch , 0\n",
    "        #if channels are less than 3, repeat channels\n",
    "        # print(x.shape)\n",
    "        # if x.shape[1] < 3:\n",
    "        #     x = x.repeat(1, 3, 1, 1)\n",
    "        # print('x in training step',x.shape)\n",
    "        if x.dim() >4: # b,16,3,h,w -> b*16,3,h,w\n",
    "            x = x.view(-1, x.shape[2], x.shape[3], x.shape[4])\n",
    "        # encode x to get the mu and variance parameters\n",
    "        x_encoded = self.encoder(x)\n",
    "        # print('x_encoded in training step',x_encoded.shape)\n",
    "        mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\n",
    "        # detect nan \n",
    "        if torch.isnan(mu).any() or torch.isnan(log_var).any():\n",
    "            print(\"NAN in mu or log_var\")\n",
    "            print(mu)\n",
    "            print(log_var)\n",
    "            print(x_encoded,x_encoded.shape)\n",
    "            raise ValueError(\"NAN in mu or log_var\")\n",
    "        # sample z from q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "\n",
    "        # decoded\n",
    "        # print('z in training step',z.shape)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        #if channels are less than 3, repeat channels\n",
    "        # print(x.shape)\n",
    "        # if x.shape[1] < 3:\n",
    "        #     x = x.repeat(1, 3, 1, 1)\n",
    "        # print(x.shape)\n",
    "        if x.dim() >4: # b,16,3,h,w -> b*16,3,h,w\n",
    "            x = x.view(-1, x.shape[2], x.shape[3], x.shape[4])\n",
    "        # encode x to get the mu and variance parameters\n",
    "        x_encoded = self.encoder(x)\n",
    "        mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\n",
    "\n",
    "        # sample z from q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "\n",
    "        # decoded\n",
    "\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss_ = self.gaussian_likelihood(x_hat, self.log_scale, x)\n",
    "        # print(recon_loss.shape)\n",
    "        recon_loss = torch.nn.MSELoss()(x_hat,x)\n",
    "\n",
    "\n",
    "        # kl\n",
    "        kl = self.kl_divergence(z, mu, std)\n",
    "\n",
    "        # elbo\n",
    "        elbo = (kl - recon_loss_) # with old recon_loss\n",
    "        # elbo = (kl + recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "\n",
    "        self.log_dict({\n",
    "            'val_elbo': elbo,\n",
    "            'val_kl': kl.mean(),\n",
    "            'val_recon_loss_': recon_loss.mean(),\n",
    "            'val_recon_loss': recon_loss_.mean(),\n",
    "            'val_reconstruction': recon_loss.mean(),\n",
    "            'val_kl': kl.mean(),\n",
    "        })\n",
    "\n",
    "        return elbo\n",
    "\n",
    "\n",
    "data , loader = get_data(pz = 112, img_size=  224)\n",
    "v = VAE(input_height=data[0][0].shape[-1],window_size =28, patch_size = 4)\n",
    "v.training_step(data[0],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#lets dfine an nn.Module to wrap the encoder and decoder\n",
    "class AE(nn.Module):\n",
    "    def __init__(self,inp):\n",
    "        super().__init__()\n",
    "        self.encoder = SwinTransformerAE(inp,4)\n",
    "        self.decoder = Resdecoder(latent_dim=256,input_height=inp,first_conv=False,maxpool1=False)\n",
    "    def init_params(self,x):\n",
    "        # print('in to encoder ',x.shape)\n",
    "        x = self.encoder.forward(x)\n",
    "        # print('out from encoder ',x[-1].shape)\n",
    "        self.decoder.init_params(x[-1])\n",
    "    def forward(self,x):\n",
    "        # print('in to encoder ',x.shape)\n",
    "        out = self.encoder.forward_features(x)\n",
    "        # print('out from encoder ',out.shape)\n",
    "        \n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "# model = AE(inp=data[0][0].shape[-1])\n",
    "# model.init_params(data[0][0])\n",
    "# model(data[0][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "class AEpl(pl.LightningModule):\n",
    "    def __init__(self,inp,lr):\n",
    "        \"\"\"This is a class for training an autoencoder with pytorch lightning.\n",
    "\n",
    "        Args:\n",
    "            inp (int): number of input features\n",
    "            lr (float): learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = AE(inp)\n",
    "        self.loss = nn.MSELoss()\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    #before train\n",
    "    def init_params(self,x):\n",
    "        self.model.init_params(x)\n",
    "    def to_cuda(self):\n",
    "        self.model.encoder.cuda()\n",
    "        self.model.decoder.cuda()\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,_ = batch\n",
    "        if len(x.shape) > 4:\n",
    "            x = x.reshape(-1,x.shape[-3],x.shape[-2],x.shape[-1])\n",
    "        out = self.forward(x)\n",
    "        loss = self.loss(x,out)\n",
    "        self.log('train_loss',loss)\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return opt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#`` lr logging\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from callbacks import  TestReconCallback_vae,TestReconCallback_ae\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "def get_callbacks():\n",
    "    callbacks = []\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "    callbacks.append(lr_monitor)\n",
    "\n",
    "    # save checkpoint on last epoch only\n",
    "    ckpt = ModelCheckpoint(f\"/home/uz1/projects/GCN/logging/swin_vae/{datetime.datetime.now().strftime('%Y_%m_%d')}\",\n",
    "                        monitor=\"elbo\",\n",
    "                        save_weights_only=True)\n",
    "    callbacks.append(ckpt)\n",
    "    return callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      " patch_size: 64 img_size: 224 num_patches: 9 bs: 10\n",
      "using input height  64  and latent dim  256  and enc_out_dim  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/ipykernel_launcher.py:81: UnderReviewWarning: The feature DecoderBlock is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transfering from  64  To  64\n",
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "VAE                                                     [1, 3, 64, 64]            1\n",
      "├─SwinTransformer: 1-1                                  [1, 512]                  --\n",
      "│    └─Sequential: 2-1                                  [1, 8, 8, 192]            --\n",
      "│    │    └─Sequential: 3-1                             [1, 16, 16, 96]           4,896\n",
      "│    │    └─Sequential: 3-2                             [1, 16, 16, 96]           455,472\n",
      "│    │    └─PatchMerging: 3-3                           [1, 8, 8, 192]            74,496\n",
      "│    │    └─Sequential: 3-4                             [1, 8, 8, 192]            2,693,520\n",
      "│    └─LayerNorm: 2-2                                   [1, 8, 8, 192]            384\n",
      "│    └─Permute: 2-3                                     [1, 192, 8, 8]            --\n",
      "│    └─AdaptiveAvgPool2d: 2-4                           [1, 192, 1, 1]            --\n",
      "│    └─Flatten: 2-5                                     [1, 192]                  --\n",
      "│    └─Linear: 2-6                                      [1, 512]                  98,816\n",
      "├─Linear: 1-2                                           [1, 256]                  131,328\n",
      "├─Linear: 1-3                                           [1, 256]                  131,328\n",
      "├─Resdecoder: 1-4                                       [1, 3, 64, 64]            --\n",
      "│    └─Linear: 2-7                                      [1, 12288]                3,158,016\n",
      "│    └─Interpolate: 2-8                                 [1, 192, 8, 8]            --\n",
      "│    └─Sequential: 2-9                                  [1, 256, 16, 16]          --\n",
      "│    │    └─DecoderBlock: 3-5                           [1, 256, 16, 16]          824,704\n",
      "│    │    └─DecoderBlock: 3-6                           [1, 256, 16, 16]          1,180,672\n",
      "│    └─Sequential: 2-10                                 [1, 128, 32, 32]          --\n",
      "│    │    └─DecoderBlock: 3-7                           [1, 128, 32, 32]          918,528\n",
      "│    │    └─DecoderBlock: 3-8                           [1, 128, 32, 32]          295,424\n",
      "│    └─Sequential: 2-11                                 [1, 64, 64, 64]           --\n",
      "│    │    └─DecoderBlock: 3-9                           [1, 64, 64, 64]           229,888\n",
      "│    │    └─DecoderBlock: 3-10                          [1, 64, 64, 64]           73,984\n",
      "│    └─Sequential: 2-12                                 [1, 64, 64, 64]           --\n",
      "│    │    └─DecoderBlock: 3-11                          [1, 64, 64, 64]           73,984\n",
      "│    │    └─DecoderBlock: 3-12                          [1, 64, 64, 64]           73,984\n",
      "│    └─Interpolate: 2-13                                [1, 64, 64, 64]           --\n",
      "│    └─Conv2d: 2-14                                     [1, 3, 64, 64]            1,728\n",
      "=========================================================================================================\n",
      "Total params: 10,421,153\n",
      "Trainable params: 10,421,153\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 2.64\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 60.10\n",
      "Params size (MB): 37.40\n",
      "Estimated Total Size (MB): 97.56\n",
      "=========================================================================================================\n",
      "config:\n",
      " patch_size: 64 img_size: 224 num_patches: 9 bs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/lightning_lite/plugins/environments/slurm.py:172: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/uz1/miniconda3/envs/ML/lib/python3.7/site-pack ...\n",
      "  category=PossibleUserWarning,\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/applications/slurm-22.05.2/bin/srun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/lightning_lite/plugins/environments/slurm.py:172: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/uz1/miniconda3/envs/ML/lib/python3.7/site-pack ...\n",
      "  category=PossibleUserWarning,\n",
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:606: UserWarning: Checkpoint directory /home/uz1/projects/GCN/logging/swin_vae/2023_03_16 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type            | Params\n",
      "--------------------------------------------\n",
      "0 | encoder | SwinTransformer | 3.3 M \n",
      "1 | fc_mu   | Linear          | 131 K \n",
      "2 | fc_var  | Linear          | 131 K \n",
      "3 | decoder | Resdecoder      | 6.8 M \n",
      "--------------------------------------------\n",
      "10.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.4 M    Total params\n",
      "41.685    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/applications/slurm-22.05.2/bin/srun\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bce10759e424220ba0431bc0ae36fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c65c7f6d320458889c4aafdb2ac4f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a099a6648d9344f4b318be720d134465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21dec7d5540e474d9c2776db1eb87b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "#data \n",
    "pz=64\n",
    "data , loader = get_data(pz = pz, img_size=  224, )\n",
    "#clbs \n",
    "# add test for mid-train recon viewing\n",
    "test = [data[x][0] for x in random.sample(range(len(data)), 1)]\n",
    "\n",
    "test = torch.stack(test, 0).squeeze()\n",
    "testRecon = TestReconCallback_vae(test)\n",
    "callbacks = get_callbacks()\n",
    "callbacks.append(testRecon)\n",
    "# train \n",
    "# model = AEpl(inp=data[0][0].shape[-1],lr=1e-3)\n",
    "inp = data[0][0].shape[-1]\n",
    "model2 = VAE(input_height=inp,window_size =7, patch_size = 4)\n",
    "model = model2\n",
    "print(\"transfering from \",model.hparams.input_height, ' To ', model2.hparams.input_height)\n",
    "#check if hparams are the same\n",
    "# param check\n",
    "\n",
    "from torchinfo import summary\n",
    "print(summary(model, input_size=(1, data[0][0].shape[1], pz, pz),))\n",
    "if model.hparams.input_height != model2.hparams.input_height:\n",
    "    # take our current patch size and resize it to the new input size\n",
    "    data,loader= get_data(pz = model2.hparams.input_height, img_size= 224,pz_=model.hparams.input_height,split='train')\n",
    "    #readjust callbacks \n",
    "    test = [data[x][0] for x in random.sample(range(len(data)), 1)]\n",
    "    callbacks = get_callbacks()\n",
    "    test = torch.stack(test, 0).squeeze()\n",
    "    testRecon = TestReconCallback_vae(test)\n",
    "    callbacks.append(testRecon)\n",
    "    print('new data size',data[0][0].shape)\n",
    "# model.to_cuda()\n",
    "_,test_loader = get_data(pz = pz, img_size=  224,split='val',pz_=model.hparams.input_height)\n",
    "# build trainer\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=10, callbacks=callbacks )\n",
    "# train\n",
    "trainer.fit(model, loader,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_2926287/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3404544689.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_2926287/3404544689.py'</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_2926287/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2457605489.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">122</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_2926287/2457605489.py'</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'tuple'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'dim'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_2926287/\u001b[0m\u001b[1;33m3404544689.py\u001b[0m:\u001b[94m7\u001b[0m in \u001b[92m<module>\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_2926287/3404544689.py'\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1194\u001b[0m in      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_2926287/\u001b[0m\u001b[1;33m2457605489.py\u001b[0m:\u001b[94m122\u001b[0m in \u001b[92mforward\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_2926287/2457605489.py'\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'tuple'\u001b[0m object has no attribute \u001b[32m'dim'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test results of recondtruction\n",
    "model.eval()\n",
    "import random\n",
    "i= random.randint(0,len(data))\n",
    "with torch.no_grad():\n",
    "    x = data[i]\n",
    "    x_hat = model(x)\n",
    "    x_hat = x_hat.squeeze(0)\n",
    "    x_hat = x_hat.cpu().numpy()\n",
    "    x = x[0].cpu().numpy()\n",
    "    x_hat = np.moveaxis(x_hat, 1, -1)\n",
    "    x = np.moveaxis(x, 1, -1)\n",
    "    print(x_hat.shape)\n",
    "\n",
    "# plot - handle multiple images , x is multiple images\n",
    "# for tht length of x.shape[0] \n",
    "for i in range(x.shape[0]):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(x[i])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(x_hat[i])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      " patch_size: 64 img_size: 224 num_patches: 9 bs: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_1624640/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">164525078.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_1624640/164525078.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'inp'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_1624640/\u001b[0m\u001b[1;33m164525078.py\u001b[0m:\u001b[94m10\u001b[0m in \u001b[92m<module>\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_1624640/164525078.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'inp'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#using mini batch kemeans \n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import math\n",
    "import pickle\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "data,loader= get_data(pz = pz,img_size= 224,pz_=None,split='val')\n",
    "num_nodes = 168\n",
    "num_points = 700 if len(data) > 20000 else len(data)\n",
    "model2 = VAE(input_height=inp,window_size =28, patch_size = 4)\n",
    "vae = model2.load_from_checkpoint(\"/home/uz1/projects/GCN/logging/swin_vae/2023_03_16/epoch=0-step=8999.ckpt\",) # 64\n",
    "h=int(int(data[0][0].shape[-1]) * sqrt(data[0][0].shape[0]))\n",
    "p_z = int(sqrt((h*h) // int(data[0][0].shape[0])))\n",
    "num_points = num_points if (((h // p_z) * (h // p_z) ) * num_points ) % 16000 == 0 else 16000 // ((h // p_z) * (h // p_z))\n",
    "num_patches= (h // p_z) * (h // p_z)\n",
    "num_points = 16000 // (num_patches * loader.batch_size)\n",
    "print(\"given num_patches: \", num_patches, \" num_points: \", num_points)\n",
    "\n",
    "vae = vae.to('cpu')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "h = 224\n",
    "p_z = data[0][0].shape[-1]\n",
    "# print(\"Using a VAE with h=\",h,\"and p(z)=\",p_z)\n",
    "if loader.batch_size < num_nodes:\n",
    "    batch_size = num_nodes\n",
    "    loader = DataLoader(data, batch_size=batch_size, drop_last=True, num_workers=14)\n",
    "if not os.path.exists(f\"/home/uz1/projects/GCN/GraphGym/run/kmeans-model-{h}-{p_z}-{num_nodes}-{data.__class__.__name__}-swin.pkl\"):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=num_nodes)\n",
    "    for i in tqdm(range(num_points),total=num_points,desc=\"Fitting Kmeans\",):\n",
    "        test,_ = next(iter(loader))\n",
    "        test = test.to(vae.device)\n",
    "        if test.dim() >4:\n",
    "            test = test.reshape(-1, test.shape[2], test.shape[3], test.shape[4]) # batch, num patches ,channel, height, width to batch, channel, height, width\n",
    "        x_encoded = vae.encoder(test)\n",
    "        mu, log_var = vae.fc_mu(x_encoded), vae.fc_var(x_encoded)\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "        z=z.detach().cpu().numpy()\n",
    "        kmeans.partial_fit(z)\n",
    "    print(\"Done - out shape \",z.shape)\n",
    "    with open(f\"/home/uz1/projects/GCN/GraphGym/run/kmeans-model-{h}-{p_z}-{num_nodes}-{data.__class__.__name__}-swin.pkl\", 'wb') as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "\n",
    "print(\"Kmeans Done !\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      " patch_size: 64 img_size: 224 num_patches: 9 bs: 10\n",
      "using input height  224  and latent dim  256  and enc_out_dim  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/ipykernel_launcher.py:81: UnderReviewWarning: The feature DecoderBlock is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using input height  64  and latent dim  256  and enc_out_dim  512\n"
     ]
    }
   ],
   "source": [
    "pz=64\n",
    "data,loader= get_data(pz = pz,img_size= 224,pz_=None,split='train')\n",
    "num_nodes = 168\n",
    "num_points = 700 if len(data) > 20000 else len(data)\n",
    "model2 = VAE(input_height=224,window_size =28, patch_size = 4)\n",
    "vae = model2.load_from_checkpoint(\"/home/uz1/projects/GCN/logging/swin_vae/2023_03_16/epoch=0-step=8999.ckpt\",) # 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def get_embedding_vae(x,vae):\n",
    "\n",
    "    x_encoded = vae.encoder(x)\n",
    "    mu, log_var = vae.fc_mu(x_encoded), vae.fc_var(x_encoded)\n",
    "    std = torch.exp(log_var / 2)\n",
    "    q = torch.distributions.Normal(mu, std)\n",
    "    z = q.rsample()\n",
    "    return z\n",
    "\n",
    "\n",
    "from torch_geometric.data import Data,Dataset\n",
    "from torch_geometric.utils import to_dense_adj, grid,dense_to_sparse\n",
    "\n",
    "def filter_a(data):\n",
    "\n",
    "    if data.y==3:\n",
    "        return False\n",
    "    else:\n",
    "        True\n",
    "def populateSBatched(labels, n_clusters=8, s=None, n_graphs=10, n_patches=16):\n",
    "    \"\"\"\n",
    "    Calculates the S cluster assignment transform of input patch features \n",
    "    for multiple graphs and returns the S and the aggregated adjacency matrix for each graph.\n",
    "    Shape : (number of patches, number of clusters)\n",
    "\n",
    "    Args:\n",
    "        labels (ndarray): Labels for each node in each graph. Shape is (n_graphs*n_patches,)\n",
    "        n_clusters (int): Number of clusters\n",
    "        s (ndarray): Existing S assignment transform to update\n",
    "        n_graphs (int): Number of graphs\n",
    "        n_patches (int): Number of patches in each image\n",
    "\n",
    "    Returns:\n",
    "        tuple: S assignment transform and aggregated adjacency matrix for each graph\n",
    "    \"\"\"\n",
    "    # n_patches = n_graphs * n_patches\n",
    "    div = int(np.sqrt(n_patches))\n",
    "\n",
    "    S_list = []\n",
    "    adj_list = []\n",
    "\n",
    "    for i in range(n_graphs):\n",
    "        start = i * n_patches\n",
    "        end = start + n_patches\n",
    "        graph_labels = labels[start:end]\n",
    "\n",
    "        s_graph = np.zeros((n_patches, n_clusters))\n",
    "        s_graph[np.arange(n_patches), graph_labels] = 1\n",
    "\n",
    "        # calc adj matrix\n",
    "        adj_graph = to_dense_adj(grid(div, div)[0]).reshape(n_patches, n_patches)\n",
    "\n",
    "        S_list.append(s_graph)\n",
    "        adj_list.append(np.matmul(np.matmul(s_graph.transpose(1, 0), adj_graph), s_graph))\n",
    "\n",
    "    S = np.concatenate(S_list)\n",
    "    adj = np.stack(adj_list)\n",
    "\n",
    "    return S, adj\n",
    "\n",
    "def populateS(labels,n_clusters=8,s=None):\n",
    "    \"\"\"\"\n",
    "    Calculates the S cluster assigment transform of input patch features \n",
    "    and returns the (S) and the aggregated (out_adj) as well.\n",
    "    shape : ( number of patches , number of clusters)\n",
    "    \"\"\"\n",
    "    # print(\"S is \" ,s==None)\n",
    "    n_patches=len(labels)\n",
    "    div = int(sqrt(n_patches))\n",
    " \n",
    "    s = np.zeros((n_patches,n_clusters))\n",
    "    s[np.arange(n_patches), labels] = 1\n",
    "         # TODO optimise this!\n",
    "\n",
    "\n",
    "    #calc adj matrix\n",
    "    adj = to_dense_adj(grid(n_patches//div,n_patches//div)[0]).reshape(n_patches,n_patches)\n",
    "    return s , np.matmul(np.matmul(s.transpose(1, 0),adj ), s)\n",
    "\n",
    "from torch_geometric.data import Data,Dataset\n",
    "from torch_geometric.utils import to_dense_adj, grid,dense_to_sparse\n",
    "\n",
    "from monai.data import GridPatchDataset, DataLoader, PatchIter\n",
    "\n",
    "\n",
    "class ImageTOGraphDatasetBatched(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset takes holds the kmaens classifier and vae encoder. On each input image we encode then get k mean label then formulate graph as Data object\n",
    "    \"\"\"\n",
    "    def __init__(self,data,vae,kmeans,norm_adj=True,return_x_only=None):\n",
    "        self.data=data\n",
    "        self.vae=vae\n",
    "        self.vae.cuda()\n",
    "        self.return_x_only=return_x_only\n",
    "\n",
    "        self.kmeans = kmeans\n",
    "        self.norm_adj = norm_adj\n",
    "        self.n_patches = self.data[1][0].shape[0]\n",
    "\n",
    "        # if self.data[1][0].dim() >3:\n",
    "        #     self.__getitem__ = self.__getitem__patches   \n",
    "            # add attr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,index):\n",
    "        #given that index is a list of indeices , load the patches and get the embedding\n",
    "        # p = self.data[index][0] wont work \n",
    "    \n",
    "        b=len(index) \n",
    "        patches = torch.concat([self.data[i][0] for i in index],0).cuda()\n",
    "        n_graphs = patches.shape[0] // self.n_patches\n",
    "        n_clusters = self.kmeans.cluster_centers_.shape[0]\n",
    "        ys = np.concatenate([self.data[i][1] for i in index],0)\n",
    "        z=get_embedding_vae(patches,self.vae.cuda()).clone().detach().cpu().numpy()\n",
    "        label=self.kmeans.predict(z)\n",
    "        s,out_adj = populateSBatched(label,n_clusters,n_graphs=n_graphs,n_patches=self.n_patches)\n",
    "\n",
    "        z = z.reshape((b, self.n_patches, -1))\n",
    "        s = s.reshape((b,self.n_patches, n_clusters))\n",
    "\n",
    "        s = np.transpose(s,(0,2,1))\n",
    "        x = np.matmul(s, z).reshape(b,n_clusters,-1)\n",
    "        del z,s,patches\n",
    "        if self.return_x_only:\n",
    "            return torch.tensor(x),torch.tensor(label),torch.tensor(ys)\n",
    "        #if normlaise adj \n",
    "        if self.norm_adj:\n",
    "            out_adj = out_adj.div(out_adj.sum(1))\n",
    "            #nan to 0 in tensor \n",
    "            out_adj = out_adj.nan_to_num(0)\n",
    "            #assert if there is nan in tensor\n",
    "            assert out_adj.isnan().any() == False , \"Found nan in out_adj\"\n",
    "        return Data(x=torch.tensor(x).float(),edge_index=dense_to_sparse(out_adj)[0],y=torch.tensor(self.data[index][1]),edge_attr=dense_to_sparse(out_adj)[1])\n",
    "class ImageTOGraphDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset takes holds the kmaens classifier and vae encoder. On each input image we encode then get k mean label then formulate graph as Data object\n",
    "    \"\"\"\n",
    "    def __init__(self,data,vae,kmeans,norm_adj=True,return_x_only=None):\n",
    "        self.data=data\n",
    "        self.vae=vae\n",
    "        self.return_x_only=return_x_only\n",
    "\n",
    "        self.kmeans = kmeans\n",
    "        self.norm_adj = norm_adj\n",
    "\n",
    "        # if self.data[1][0].dim() >3:\n",
    "        #     self.__getitem__ = self.__getitem__patches   \n",
    "            # add attr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        patches = self.data[index][0]\n",
    "        z=get_embedding_vae(patches.cuda(),self.vae.cuda()).clone().detach().cpu().numpy()\n",
    "        label=self.kmeans.predict(z)\n",
    "        s,out_adj = populateS(label,self.kmeans.cluster_centers_.shape[0])\n",
    "        x = np.matmul(s.transpose(1,0) , z)\n",
    "        del z\n",
    "        del s,patches\n",
    "        if self.return_x_only:\n",
    "            return torch.tensor(x),torch.tensor(label),torch.tensor(self.data[index][1])\n",
    "        #if normlaise adj \n",
    "        if self.norm_adj:\n",
    "            out_adj = out_adj.div(out_adj.sum(1))\n",
    "            #nan to 0 in tensor \n",
    "            out_adj = out_adj.nan_to_num(0)\n",
    "            #assert if there is nan in tensor\n",
    "            assert out_adj.isnan().any() == False , \"Found nan in out_adj\"\n",
    "        return Data(x=torch.tensor(x).float(),edge_index=dense_to_sparse(out_adj)[0],y=torch.tensor(self.data[index][1]),edge_attr=dense_to_sparse(out_adj)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load /home/uz1/projects/GCN/GraphGym/run/kmeans-model-{h}-{p_z}-{num_nodes}-{data.__class__.__name__}-swin.pkl\n",
    "h=224\n",
    "import pickle\n",
    "with open(f\"/home/uz1/projects/GCN/GraphGym/run/kmeans-model-{h}-{pz}-{num_nodes}-{data.__class__.__name__}-swin.pkl\",\"rb\") as f:\n",
    "    k = pickle.load(f)\n",
    "ImData = ImageTOGraphDatasetBatched(data=data,vae=vae,kmeans=k,return_x_only=True)\n",
    "sampler = torch.utils.data.sampler.BatchSampler(\n",
    "    torch.utils.data.sampler.RandomSampler(ImData),\n",
    "    batch_size=loader.batch_size,\n",
    "    drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters: 168, embed_size: 256, n_patches: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9000/9000 [1:01:38<00:00,  2.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# train_loader = DataLoader(ImData, batch_size=args.batch_size,)\n",
    "train_loader = DataLoader(ImData, batch_size=1,sampler = sampler)\n",
    "# for all data in the dataset: create a h5y file to store the data and save to disk\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data,Dataset\n",
    "from tqdm import tqdm\n",
    "#create h5py file\n",
    "# if os.path.exists(f'/home/uz1/projects/GCN/graph_data/graph-data---{args.dataset}-{args.patch_size}-{args.nclusters}-{args.image_transform_size}-UC_{args.use_combined}.h5'):\n",
    "    # print(\"File already exists at \", f'/home/uz1/projects/GCN/graph_data/graph-data---{args.dataset}-{args.patch_size}-{args.nclusters}-{args.image_transform_size}-UC_{args.use_combined}.h5')\n",
    "\n",
    "    #exit\n",
    "    # exit(\"File already exists\")\n",
    "\n",
    "h5f = h5py.File(f'/home/uz1/projects/GCN/graph_data/graph-data---pathmnist-{pz}-{num_nodes}-{h}-swin_.h5', 'w')\n",
    "\n",
    "#create dataset in file\n",
    "\n",
    "h5f.create_dataset('x', (len(ImData),num_nodes,vae.fc_mu.out_features))\n",
    "\n",
    "# add the data to h5py file\n",
    "n_clusters = num_nodes\n",
    "embed_size = vae.fc_mu.out_features\n",
    "n_patches = data[0][0].shape[0]\n",
    "print(f\"n_clusters: {n_clusters}, embed_size: {embed_size}, n_patches: {n_patches}\")\n",
    "#try numpy array\n",
    "edge_i = []\n",
    "edge_at = []\n",
    "ys=[]\n",
    "tmp=0\n",
    "for i,d in tqdm(enumerate(train_loader),total=len(train_loader)):\n",
    "\n",
    "    h5f['x'][tmp:tmp+d[0].squeeze().shape[0]] = d[0].reshape(-1,n_clusters,embed_size)\n",
    "    tmp=tmp+d[0].squeeze().shape[0]\n",
    "\n",
    "    ys.append(d[2].squeeze())\n",
    "    edge_i.append(d[1].squeeze())\n",
    "\n",
    "edge_i = np.stack(edge_i[:-1])\n",
    "edge_i = edge_i.reshape(-1,ImData[[0]][1].shape[0])\n",
    "ys = np.stack(ys[:-1])\n",
    "ys = np.concatenate([ys.reshape(-1),d[2].reshape(-1)]) \n",
    "edge_i = np.concatenate([edge_i.reshape(-1,n_patches),d[1].reshape(-1,n_patches)])\n",
    "\n",
    "#create datasets for edge index and edge attr and ys\n",
    "h5f.create_dataset('edge_index', data=edge_i)\n",
    "h5f.create_dataset('ys', data=ys)\n",
    "\n",
    "#close the file\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89996, 9)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_i.reshape(-1,ImData[[0]][1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270024, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([edge_i.reshape(-1,n_patches),d[1].reshape(-1,n_patches)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3, 64, 64])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bf7c8448272c4cbdce3f78384e0b31dc492bbd9290e96311fca142ad432e9ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
